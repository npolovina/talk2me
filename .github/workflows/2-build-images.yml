name: 2. Build and Deploy Application

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name'
        required: true
        default: 'talk2me-cluster'
      image_tag:
        description: 'Docker Image Tag'
        required: true
        default: 'latest'
      domain_name:
        description: 'Domain Name'
        required: true
        default: 'talk2me-gen-z.com'

env:
  AWS_REGION: ${{ github.event.inputs.region }}
  CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
  IMAGE_TAG: ${{ github.event.inputs.image_tag }}
  DOMAIN_NAME: ${{ github.event.inputs.domain_name }}
  API_DOMAIN_NAME: api.${{ github.event.inputs.domain_name }}
  BACKEND_ECR_REPOSITORY: talk2me-backend
  FRONTEND_ECR_REPOSITORY: talk2me-frontend
  ALB_NAME: talk2me-alb

permissions:
  id-token: write
  contents: read

jobs:
  build-and-push-images:
    name: Build and Push Docker Images
    runs-on: ubuntu-latest
    
    outputs:
      backend_image: ${{ steps.build-backend.outputs.image }}
      frontend_image: ${{ steps.build-frontend.outputs.image }}
      aws_account_id: ${{ steps.get-aws-account.outputs.account_id }}
      sha_tag: ${{ steps.sha-tag.outputs.sha_short }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Get AWS Account ID
        id: get-aws-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV
          echo "account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT
      
      - name: Generate short SHA for tag
        id: sha-tag
        run: |
          SHORT_SHA="${GITHUB_SHA:0:7}"
          echo "Creating image tag with: $SHORT_SHA"
          echo "sha_short=$SHORT_SHA" >> $GITHUB_OUTPUT
          
          # Create a combined tag with user input and SHA
          if [ "$IMAGE_TAG" == "latest" ]; then
            # If user specified "latest", just use the SHA
            COMBINED_TAG="$SHORT_SHA"
          else
            # Otherwise combine user tag and SHA
            COMBINED_TAG="${IMAGE_TAG}-${SHORT_SHA}"
          fi
          
          echo "COMBINED_TAG=$COMBINED_TAG" >> $GITHUB_ENV
          echo "combined_tag=$COMBINED_TAG" >> $GITHUB_OUTPUT
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Build and Push Backend Image
        id: build-backend
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          # Find the backend Dockerfile
          BACKEND_DOCKERFILE=$(find . -type f -name "Dockerfile" -path "*/backend/*" | head -1)
          
          if [ -z "$BACKEND_DOCKERFILE" ]; then
            echo "Error: Could not find a Dockerfile in the backend directory"
            exit 1
          fi
          
          BACKEND_DIR=$(dirname "$BACKEND_DOCKERFILE")
          echo "Found backend Dockerfile at: $BACKEND_DOCKERFILE"
          echo "Building from directory: $BACKEND_DIR"
          
          # Generate a unique build timestamp
          BUILD_TIMESTAMP=$(date +%s)
          
          # Create temporary Dockerfile with ARG to invalidate cache
          cat > "${BACKEND_DIR}/Dockerfile.timestamp" << EOF
          # Add build timestamp to invalidate cache
          ARG BUILD_TIMESTAMP=$BUILD_TIMESTAMP
          
          $(cat "$BACKEND_DOCKERFILE")
          EOF
          
          # Build with both the user-specified tag and the SHA tag, forcing a clean build
          docker build --no-cache --pull --force-rm \
            --build-arg BUILD_TIMESTAMP=$BUILD_TIMESTAMP \
            -f "${BACKEND_DIR}/Dockerfile.timestamp" \
            -t $ECR_REGISTRY/$BACKEND_ECR_REPOSITORY:$IMAGE_TAG \
            -t $ECR_REGISTRY/$BACKEND_ECR_REPOSITORY:$COMBINED_TAG \
            "$BACKEND_DIR"
          
          # Clean any existing images with these tags in ECR (ignore errors if they don't exist)
          aws ecr batch-delete-image \
            --repository-name $BACKEND_ECR_REPOSITORY \
            --image-ids imageTag=$IMAGE_TAG imageTag=$COMBINED_TAG 2>/dev/null || true
          
          # Push both tags
          docker push $ECR_REGISTRY/$BACKEND_ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$BACKEND_ECR_REPOSITORY:$COMBINED_TAG
          
          # Use the combined tag for deployment
          echo "image=$ECR_REGISTRY/$BACKEND_ECR_REPOSITORY:$COMBINED_TAG" >> $GITHUB_OUTPUT
      
      - name: Build and Push Frontend Image
        id: build-frontend
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          REACT_APP_API_URL: https://${{ env.API_DOMAIN_NAME }}
        run: |
          # Find the frontend Dockerfile
          FRONTEND_DOCKERFILE=$(find . -type f -name "Dockerfile" -path "*/frontend/*" | head -1)
          
          if [ -z "$FRONTEND_DOCKERFILE" ]; then
            echo "Error: Could not find a Dockerfile in the frontend directory"
            exit 1
          fi
          
          FRONTEND_DIR=$(dirname "$FRONTEND_DOCKERFILE")
          echo "Found frontend Dockerfile at: $FRONTEND_DOCKERFILE"
          echo "Building from directory: $FRONTEND_DIR"
          
          # Generate a unique build timestamp
          BUILD_TIMESTAMP=$(date +%s)
          
          # Create temporary Dockerfile with ARG to invalidate cache
          cat > "${FRONTEND_DIR}/Dockerfile.timestamp" << EOF
          # Add build timestamp to invalidate cache
          ARG BUILD_TIMESTAMP=$BUILD_TIMESTAMP
          
          $(cat "$FRONTEND_DOCKERFILE")
          EOF
          
          # Build with both the user-specified tag and the SHA tag, forcing a clean build
          docker build --no-cache --pull --force-rm \
            --build-arg BUILD_TIMESTAMP=$BUILD_TIMESTAMP \
            --build-arg REACT_APP_API_URL=$REACT_APP_API_URL \
            -f "${FRONTEND_DIR}/Dockerfile.timestamp" \
            -t $ECR_REGISTRY/$FRONTEND_ECR_REPOSITORY:$IMAGE_TAG \
            -t $ECR_REGISTRY/$FRONTEND_ECR_REPOSITORY:$COMBINED_TAG \
            "$FRONTEND_DIR"
          
          # Clean any existing images with these tags in ECR (ignore errors if they don't exist)
          aws ecr batch-delete-image \
            --repository-name $FRONTEND_ECR_REPOSITORY \
            --image-ids imageTag=$IMAGE_TAG imageTag=$COMBINED_TAG 2>/dev/null || true
          
          # Push both tags
          docker push $ECR_REGISTRY/$FRONTEND_ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$FRONTEND_ECR_REPOSITORY:$COMBINED_TAG
          
          # Use the combined tag for deployment
          echo "image=$ECR_REGISTRY/$FRONTEND_ECR_REPOSITORY:$COMBINED_TAG" >> $GITHUB_OUTPUT
      
      - name: Summary of Build Step
        run: |
          echo "==== Docker Images Built and Pushed ===="
          echo "Commit SHA: ${GITHUB_SHA}"
          echo "Short SHA Tag: ${{ steps.sha-tag.outputs.sha_short }}"
          echo "Combined Tag: $COMBINED_TAG"
          echo "Backend Image: ${{ steps.build-backend.outputs.image }}"
          echo "Frontend Image: ${{ steps.build-frontend.outputs.image }}"
          echo "These images will be used in the deployment step"
  
  deploy-to-eks:
    name: Deploy to EKS
    needs: build-and-push-images
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
      
      - name: Get ACM Certificate ARN
        id: get-certificate
        run: |
          echo "Finding ACM certificate for $DOMAIN_NAME..."
          
          # Try to find certificate for domain in ACM
          CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?contains(DomainName, '*.${DOMAIN_NAME}') || DomainName=='${DOMAIN_NAME}'].CertificateArn" --output text | head -1)
          
          if [ -n "$CERT_ARN" ]; then
            echo "Found certificate: $CERT_ARN"
            echo "CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV
            echo "certificate_found=true" >> $GITHUB_OUTPUT
          else
            echo "No certificate found for $DOMAIN_NAME"
            echo "certificate_found=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Create Backend Deployment Template
        run: |
          # Create directory for Kubernetes manifests
          mkdir -p k8s
          
          # Set variables for image references
          BACKEND_IMAGE="${{ needs.build-and-push-images.outputs.aws_account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.BACKEND_ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"
          
          # Create backend deployment manifest
          cat > k8s/backend-deployment.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: talk2me-backend
            namespace: talk2me
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: talk2me-backend
            template:
              metadata:
                labels:
                  app: talk2me-backend
              spec:
                containers:
                - name: backend
                  image: $BACKEND_IMAGE
                  ports:
                  - containerPort: 8000
                  env:
                  - name: DEEPSEEK_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: talk2me-secrets
                        key: deepseek-api-key
                  resources:
                    requests:
                      memory: "256Mi"
                      cpu: "100m"
                    limits:
                      memory: "512Mi"
                      cpu: "500m"
                  readinessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 30
                    periodSeconds: 15
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 6
                  livenessProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 60
                    periodSeconds: 20
                    timeoutSeconds: 5
                    failureThreshold: 6
                  startupProbe:
                    httpGet:
                      path: /health
                      port: 8000
                    initialDelaySeconds: 10
                    periodSeconds: 10
                    timeoutSeconds: 5
                    failureThreshold: 30
          EOF
          
          # Create backend service manifest
          cat > k8s/backend-service.yaml << EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: talk2me-backend
            namespace: talk2me
          spec:
            selector:
              app: talk2me-backend
            ports:
            - port: 80
              targetPort: 8000
            type: ClusterIP
          EOF
      
      - name: Create Frontend Deployment Template
        run: |
          # Set variables for image references - using the same tag as backend for consistency
          FRONTEND_IMAGE="${{ needs.build-and-push-images.outputs.aws_account_id }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.FRONTEND_ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}"
          
          # Create frontend deployment manifest
          cat > k8s/frontend-deployment.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: talk2me-frontend
            namespace: talk2me
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: talk2me-frontend
            template:
              metadata:
                labels:
                  app: talk2me-frontend
              spec:
                containers:
                - name: frontend
                  image: $FRONTEND_IMAGE
                  ports:
                  - containerPort: 80
                  resources:
                    requests:
                      memory: "128Mi"
                      cpu: "100m"
                    limits:
                      memory: "256Mi"
                      cpu: "300m"
                  readinessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 10
                    periodSeconds: 10
                    timeoutSeconds: 5
                    successThreshold: 1
                    failureThreshold: 3
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 20
                    periodSeconds: 20
                    timeoutSeconds: 5
                    failureThreshold: 6
          EOF
          
          # Create frontend service manifest
          cat > k8s/frontend-service.yaml << EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: talk2me-frontend
            namespace: talk2me
          spec:
            selector:
              app: talk2me-frontend
            ports:
            - port: 80
              targetPort: 80
            type: ClusterIP
          EOF
      
      - name: Create Ingress Configuration
        run: |
          # Create ingress manifest with common annotations
          cat > k8s/ingress.yaml << EOF
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: talk2me-ingress
            namespace: talk2me
            annotations:
              kubernetes.io/ingress.class: "alb"
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/load-balancer-name: ${{ env.ALB_NAME }}
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
              alb.ingress.kubernetes.io/ssl-redirect: '443'
              alb.ingress.kubernetes.io/ssl-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
              external-dns.alpha.kubernetes.io/hostname: "${{ env.DOMAIN_NAME }},${{ env.API_DOMAIN_NAME }}"
              alb.ingress.kubernetes.io/group.name: talk2me
          EOF
          
          # Add certificate ARN if available
          if [ "${{ steps.get-certificate.outputs.certificate_found }}" == "true" ]; then
            echo "    alb.ingress.kubernetes.io/certificate-arn: ${CERTIFICATE_ARN}" >> k8s/ingress.yaml
          fi
          
          # Add rules to ingress
          cat >> k8s/ingress.yaml << EOF
          spec:
            rules:
            - host: ${{ env.DOMAIN_NAME }}
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: talk2me-frontend
                      port:
                        number: 80
            - host: ${{ env.API_DOMAIN_NAME }}
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: talk2me-backend
                      port:
                        number: 80
          EOF
      
      - name: Apply Kubernetes Configurations
        run: |
          # Make sure namespace exists
          kubectl create namespace talk2me --dry-run=client -o yaml | kubectl apply -f -
          
          # Apply secrets configuration from template
          if [ -f "k8s/secrets.yaml" ]; then
            echo "Applying secrets from template file..."
            # Encode the API key to base64
            DEEPSEEK_API_KEY_BASE64=$(echo -n "${{ secrets.DEEPSEEK_API_KEY }}" | base64)
            # Use sed to replace the placeholder with the actual value
            sed "s|\${DEEPSEEK_API_KEY_BASE64}|$DEEPSEEK_API_KEY_BASE64|g" k8s/secrets.yaml | kubectl apply -f -
          else
            echo "Creating secret directly..."
            # Create secret directly if template doesn't exist
            kubectl create secret generic talk2me-secrets \
              --namespace talk2me \
              --from-literal=deepseek-api-key="${{ secrets.DEEPSEEK_API_KEY }}" \
              --dry-run=client -o yaml | kubectl apply -f -
          fi
          
          # Apply backend configurations
          kubectl apply -f k8s/backend-deployment.yaml
          kubectl apply -f k8s/backend-service.yaml
          
          # Apply frontend configurations
          kubectl apply -f k8s/frontend-deployment.yaml
          kubectl apply -f k8s/frontend-service.yaml
          
          # Apply ingress configuration
          kubectl apply -f k8s/ingress.yaml
      
      - name: Wait for deployments to be ready
        id: wait-deployments
        continue-on-error: true
        run: |
          echo "Checking backend deployment status first..."
          kubectl get deployment talk2me-backend -n talk2me -o wide || echo "Backend deployment not found yet"
          
          echo "Waiting for backend deployment to be ready (600s timeout)..."
          kubectl rollout status deployment/talk2me-backend -n talk2me --timeout=600s || echo "Backend deployment timed out or not ready"
          
          echo "Checking frontend deployment status..."
          kubectl get deployment talk2me-frontend -n talk2me -o wide || echo "Frontend deployment not found yet"
          
          echo "Waiting for frontend deployment to be ready (600s timeout)..."
          kubectl rollout status deployment/talk2me-frontend -n talk2me --timeout=600s || echo "Frontend deployment timed out or not ready"
          
          # Continue the workflow regardless of deployment status
          echo "Proceeding with workflow regardless of deployment status"
      
      - name: Check deployment status before proceeding
        run: |
          echo "WARNING: Checking deployment status..."
          
          # Get pod statuses directly without relying on outputs from previous steps
          BACKEND_PODS=$(kubectl get pods -n talk2me -l app=talk2me-backend -o jsonpath='{.items[*].status.phase}')
          FRONTEND_PODS=$(kubectl get pods -n talk2me -l app=talk2me-frontend -o jsonpath='{.items[*].status.phase}')
          
          echo "Backend pods status: $BACKEND_PODS"
          echo "Frontend pods status: $FRONTEND_PODS"
          
          # Check if all backend pods are 'Running'
          if [[ "$BACKEND_PODS" == *"Running"* ]] && [[ ! "$BACKEND_PODS" == *"Pending"* ]] && [[ ! "$BACKEND_PODS" == *"Failed"* ]]; then
            echo "Backend status: READY"
            BACKEND_READY="true"
          else
            echo "Backend status: NOT READY"
            BACKEND_READY="false"
          fi
          
          # Check if all frontend pods are 'Running'
          if [[ "$FRONTEND_PODS" == *"Running"* ]] && [[ ! "$FRONTEND_PODS" == *"Pending"* ]] && [[ ! "$FRONTEND_PODS" == *"Failed"* ]]; then
            echo "Frontend status: READY"
            FRONTEND_READY="true"
          else
            echo "Frontend status: NOT READY"
            FRONTEND_READY="false"
          fi
          
          # Check for any potential issues if pods aren't ready
          if [[ "$BACKEND_READY" == "false" ]] || [[ "$FRONTEND_READY" == "false" ]]; then
            echo "Continuing with ingress setup, but the application may not be fully functional."
            echo "Checking for common deployment issues..."
            
            # Check if pods were created
            echo "Pod status:"
            kubectl get pods -n talk2me -o wide
            
            # Check node capacity
            echo "Node status:"
            kubectl get nodes
            
            # Check for any events
            echo "Recent events:"
            kubectl get events -n talk2me --sort-by='.lastTimestamp' | tail -20
            
            # Check for any errors in logs of non-running pods
            echo "Checking logs of problematic pods:"
            
            # Get failing pods
            FAILING_PODS=$(kubectl get pods -n talk2me -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}')
            
            if [ -n "$FAILING_PODS" ]; then
              echo "Found failing pods: $FAILING_PODS"
              for pod in $FAILING_PODS; do
                echo "==== Details for pod $pod ===="
                kubectl describe pod $pod -n talk2me
                echo "==== Logs from $pod (if available) ===="
                kubectl logs $pod -n talk2me --previous || kubectl logs $pod -n talk2me || echo "No logs available"
              done
            fi
            
            # Check secrets
            echo "Verifying secrets:"
            kubectl get secrets -n talk2me
            
            # Check if secret exists but don't show the value
            if kubectl get secret talk2me-secrets -n talk2me &>/dev/null; then
              echo "talk2me-secrets exists in the namespace"
            else
              echo "ERROR: talk2me-secrets doesn't exist in the namespace - this could cause startup failures"
            fi
          fi
      
      - name: Wait for ingress and load balancer
        id: get-lb
        run: |
          echo "Waiting for ingress to get an address (this may take a few minutes)..."
          
          # Check initial ingress status
          kubectl describe ingress talk2me-ingress -n talk2me
          
          # Wait for the ingress to get an address
          ATTEMPTS=0
          MAX_ATTEMPTS=45
          SLEEP_SECONDS=20
          
          while [ $ATTEMPTS -lt $MAX_ATTEMPTS ]; do
            LB_ADDRESS=$(kubectl get ingress talk2me-ingress -n talk2me -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ -n "$LB_ADDRESS" ]; then
              echo "Ingress load balancer is available at: $LB_ADDRESS"
              echo "LB_ADDRESS=$LB_ADDRESS" >> $GITHUB_ENV
              echo "lb_address=$LB_ADDRESS" >> $GITHUB_OUTPUT
              break
            fi
            
            echo "Waiting for load balancer address... Attempt $(($ATTEMPTS+1))/$MAX_ATTEMPTS"
            
            # Check for potential issues every few attempts
            if [ $(($ATTEMPTS % 5)) -eq 0 ]; then
              echo "-----------------------------------"
              echo "Checking ingress status:"
              kubectl describe ingress talk2me-ingress -n talk2me
              
              echo "-----------------------------------"
              echo "Checking AWS Load Balancer Controller logs:"
              kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=20 || echo "Failed to get logs"
            fi
            
            ATTEMPTS=$((ATTEMPTS+1))
            sleep $SLEEP_SECONDS
          done
          
          if [ -z "$LB_ADDRESS" ]; then
            echo "Warning: Load balancer address not available after several attempts"
            
            # Try to find the ALB directly in AWS
            AWS_ALB_DNS=$(aws elbv2 describe-load-balancers --names ${{ env.ALB_NAME }} --query 'LoadBalancers[0].DNSName' --output text 2>/dev/null || echo "")
            
            if [ -n "$AWS_ALB_DNS" ] && [ "$AWS_ALB_DNS" != "None" ]; then
              echo "Found ALB in AWS: $AWS_ALB_DNS"
              echo "LB_ADDRESS=$AWS_ALB_DNS" >> $GITHUB_ENV
              echo "lb_address=$AWS_ALB_DNS" >> $GITHUB_OUTPUT
            else
              echo "Could not find ALB in AWS."
              echo "lb_address=not_found" >> $GITHUB_OUTPUT
            fi
          fi
      
      - name: Setup Route53 DNS if needed
        if: steps.get-lb.outputs.lb_address != 'not_found'
        run: |
          echo "Checking if Route53 hosted zone exists for $DOMAIN_NAME..."
          
          ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $DOMAIN_NAME --query "HostedZones[?Name=='$DOMAIN_NAME.'].Id" --output text | cut -d/ -f3)
          
          if [ -z "$ZONE_ID" ]; then
            echo "Creating new hosted zone for $DOMAIN_NAME"
            
            ZONE_RESULT=$(aws route53 create-hosted-zone \
              --name $DOMAIN_NAME \
              --caller-reference "talk2me-$(date +%s)" \
              --hosted-zone-config Comment="Hosted zone for Talk2Me application")
            
            # Extract zone ID from response
            ZONE_ID=$(echo $ZONE_RESULT | jq -r '.HostedZone.Id' | cut -d/ -f3)
            echo "Created new hosted zone with ID: $ZONE_ID"
            
            # Output nameservers for domain configuration
            echo "Please configure your domain registrar with the following nameservers:"
            echo $ZONE_RESULT | jq -r '.DelegationSet.NameServers[]' | sed 's/^/  - /'
          else
            echo "Hosted zone exists with ID: $ZONE_ID"
          fi
          
          echo "ZONE_ID=$ZONE_ID" >> $GITHUB_ENV
          
          # Get the ALB hosted zone ID
          ALB_HOSTED_ZONE_ID=$(aws elbv2 describe-load-balancers --names ${{ env.ALB_NAME }} --query 'LoadBalancers[0].CanonicalHostedZoneId' --output text 2>/dev/null || echo "Z35SXDOTRQ7X7K")
          
          # If we couldn't get the hosted zone ID, use the default for us-east-1
          if [ -z "$ALB_HOSTED_ZONE_ID" ] || [ "$ALB_HOSTED_ZONE_ID" == "None" ]; then
            ALB_HOSTED_ZONE_ID="Z35SXDOTRQ7X7K"  # Default for us-east-1
          fi
          
          # Create JSON for DNS change batch
          cat > dns-changes.json << EOF
          {
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${{ env.DOMAIN_NAME }}",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_HOSTED_ZONE_ID",
                    "DNSName": "$LB_ADDRESS",
                    "EvaluateTargetHealth": true
                  }
                }
              },
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${{ env.API_DOMAIN_NAME }}",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_HOSTED_ZONE_ID",
                    "DNSName": "$LB_ADDRESS",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }
          EOF
          
          # Apply the DNS changes
          aws route53 change-resource-record-sets \
            --hosted-zone-id $ZONE_ID \
            --change-batch file://dns-changes.json
          
          echo "DNS records have been configured for ${{ env.DOMAIN_NAME }} and ${{ env.API_DOMAIN_NAME }}"
          echo "DNS changes take time to propagate. Please allow 5-10 minutes."
      
      - name: Deployment Summary
        run: |
          echo "=========== Deployment Summary ==========="
          echo "Application deployed to EKS cluster: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo "Image tag: $IMAGE_TAG"
          
          # Check all resources
          echo ""
          echo "Kubernetes resources:"
          echo "- Deployments:"
          kubectl get deployments -n talk2me
          
          echo "- Services:"
          kubectl get services -n talk2me
          
          echo "- Ingress:"
          kubectl get ingress -n talk2me
          
          echo "- Pods:"
          kubectl get pods -n talk2me
          
          # Show access information
          echo ""
          echo "Access Information:"
          if [ -n "$LB_ADDRESS" ]; then
            echo "Load Balancer Address: $LB_ADDRESS"
            echo "Frontend URL: https://${{ env.DOMAIN_NAME }}"
            echo "Backend API URL: https://${{ env.API_DOMAIN_NAME }}"
          else
            echo "Load Balancer Address: Not available"
            echo "Please check AWS Console for more information."
          fi
          
          # Note about DNS propagation
          if [ -n "$ZONE_ID" ] && [ -n "$LB_ADDRESS" ]; then
            echo ""
            echo "DNS records have been updated, but changes may take time to propagate."
            echo "If the application is not accessible immediately, please wait 5-10 minutes."
          fi