name: 1. EKS Cluster Setup

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name'
        required: true
        default: 'talk2me-cluster'
      node_group_size:
        description: 'Node Group Size (min,desired,max)'
        required: true
        default: '2,3,4'
      node_instance_type:
        description: 'EC2 Instance Type for Nodes'
        required: true
        default: 't3.medium'
      kubernetes_version:
        description: 'Kubernetes Version'
        required: true
        default: '1.28'
      enable_autoscaler:
        description: 'Enable Cluster Autoscaler'
        required: false
        default: 'false'
        type: boolean

env:
  AWS_REGION: ${{ github.event.inputs.region }}
  CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}

permissions:
  id-token: write
  contents: read

jobs:
  eks-setup:
    name: Create EKS Cluster
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Install eksctl
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version
      
      - name: Create EKS cluster
        run: |
          # Parse node group size parameters
          IFS=',' read -ra SIZES <<< "${{ github.event.inputs.node_group_size }}"
          MIN_SIZE=${SIZES[0]}
          DESIRED_SIZE=${SIZES[1]}
          MAX_SIZE=${SIZES[2]}
          
          echo "Creating EKS cluster $CLUSTER_NAME in $AWS_REGION..."
          eksctl create cluster \
            --name $CLUSTER_NAME \
            --region $AWS_REGION \
            --version ${{ github.event.inputs.kubernetes_version }} \
            --nodegroup-name standard-nodes \
            --node-type ${{ github.event.inputs.node_instance_type }} \
            --nodes-min $MIN_SIZE \
            --nodes $DESIRED_SIZE \
            --nodes-max $MAX_SIZE \
            --with-oidc \
            --managed \
            --asg-access
            
      - name: Update all EKS add-ons to compatible versions
        run: |
          echo "Updating all EKS add-ons to compatible versions for EKS 1.28..."
          
          # Ensure kubectl can connect to the cluster
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
          
          # Define the add-ons to ensure are properly configured
          ADDONS=("coredns" "kube-proxy" "vpc-cni" "aws-ebs-csi-driver")
          
          for ADDON in "${ADDONS[@]}"; do
            echo "Processing add-on: $ADDON"
            
            # Get latest compatible version for the add-on
            LATEST_VERSION=$(aws eks describe-addon-versions \
              --addon-name $ADDON \
              --kubernetes-version 1.28 \
              --query "addons[].addonVersions[0].addonVersion" \
              --output text)
              
            if [ -z "$LATEST_VERSION" ] || [ "$LATEST_VERSION" == "None" ]; then
              echo "No version found for $ADDON, skipping..."
              continue
            fi
            
            echo "Latest compatible $ADDON version: $LATEST_VERSION"
            
            # Check if add-on is already installed
            ADDON_STATUS=$(aws eks describe-addon \
              --cluster-name $CLUSTER_NAME \
              --addon-name $ADDON \
              --query "addon.status" \
              --output text 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$ADDON_STATUS" == "NOT_FOUND" ]; then
              echo "Creating $ADDON add-on with version $LATEST_VERSION"
              aws eks create-addon \
                --cluster-name $CLUSTER_NAME \
                --addon-name $ADDON \
                --addon-version $LATEST_VERSION
            else
              echo "Updating $ADDON add-on to version $LATEST_VERSION"
              aws eks update-addon \
                --cluster-name $CLUSTER_NAME \
                --addon-name $ADDON \
                --addon-version $LATEST_VERSION \
                --resolve-conflicts PRESERVE
            fi
            
            # For add-ons that have deployments, wait for them to be ready
            if [ "$ADDON" == "coredns" ]; then
              echo "Waiting for CoreDNS to be ready..."
              kubectl -n kube-system rollout status deployment coredns --timeout=180s
            elif [ "$ADDON" == "kube-proxy" ]; then
              echo "Waiting for kube-proxy to be ready..."
              kubectl -n kube-system rollout status daemonset kube-proxy --timeout=180s
            elif [ "$ADDON" == "aws-ebs-csi-driver" ]; then
              echo "Waiting for EBS CSI driver to be ready..."
              kubectl -n kube-system rollout status deployment ebs-csi-controller --timeout=180s
            fi
          done
          
          echo "All add-ons have been configured"
      
      - name: Tag subnets for Load Balancer Controller
        run: |
          echo "Tagging subnets for AWS Load Balancer Controller..."
          
          # Get VPC ID used by the EKS cluster
          VPC_ID=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "EKS Cluster VPC ID: $VPC_ID"
          
          # Get all subnets in the VPC
          SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query "Subnets[].SubnetId" --output text)
          
          # Identify public subnets (those with route to internet gateway)
          for SUBNET_ID in $SUBNET_IDS; do
            # Check if this subnet's route table has an internet gateway route
            RT_ID=$(aws ec2 describe-route-tables --filters "Name=association.subnet-id,Values=$SUBNET_ID" --query "RouteTables[0].RouteTableId" --output text)
            
            if [ -z "$RT_ID" ] || [ "$RT_ID" == "None" ]; then
              # If there's no explicit association, get the main route table for the VPC
              RT_ID=$(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID" "Name=association.main,Values=true" --query "RouteTables[0].RouteTableId" --output text)
            fi
            
            # Check if this route table has an internet gateway route
            IGW_ROUTE=$(aws ec2 describe-route-tables --route-table-ids $RT_ID --query "RouteTables[0].Routes[?GatewayId!=null] | [?starts_with(GatewayId, 'igw-')].DestinationCidrBlock" --output text)
            
            if [ -n "$IGW_ROUTE" ]; then
              # This subnet has a route to an internet gateway - it's public
              echo "Tagging public subnet $SUBNET_ID with kubernetes.io/role/elb=1"
              aws ec2 create-tags --resources $SUBNET_ID --tags Key=kubernetes.io/role/elb,Value=1
              
              # Check if the subnet also has the cluster tag, if so, also tag it for internal ELBs
              CLUSTER_TAG=$(aws ec2 describe-subnets --subnet-ids $SUBNET_ID --query "Subnets[0].Tags[?Key==\`kubernetes.io/cluster/$CLUSTER_NAME\`].Value" --output text)
              if [ -n "$CLUSTER_TAG" ]; then
                echo "Tagging cluster subnet $SUBNET_ID with kubernetes.io/role/internal-elb=1"
                aws ec2 create-tags --resources $SUBNET_ID --tags Key=kubernetes.io/role/internal-elb,Value=1
              fi
            else
              # This subnet doesn't have a route to an internet gateway - it's private
              echo "Tagging private subnet $SUBNET_ID with kubernetes.io/role/internal-elb=1"
              aws ec2 create-tags --resources $SUBNET_ID --tags Key=kubernetes.io/role/internal-elb,Value=1
            fi
          done
            
      - name: Install AWS Load Balancer Controller
        run: |
          # Create IAM policy for AWS Load Balancer Controller
          curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.2/docs/install/iam_policy.json
          
          # Create the policy
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam_policy.json || echo "Policy may already exist - continuing"

          # Create Service Account
          eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --approve
          
          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
          
          # Add Helm repository
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          # Install AWS Load Balancer Controller
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=$AWS_REGION \
            -n kube-system
          
          # Wait for controller to be ready
          echo "Waiting for AWS Load Balancer Controller to be ready..."
          kubectl rollout status deployment aws-load-balancer-controller -n kube-system --timeout=180s
      
      - name: Setup External DNS (for Route53)
        run: |
          # Create IAM policy for External DNS
          cat > external-dns-policy.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": [
                  "route53:ChangeResourceRecordSets"
                ],
                "Resource": [
                  "arn:aws:route53:::hostedzone/*"
                ]
              },
              {
                "Effect": "Allow",
                "Action": [
                  "route53:ListHostedZones",
                  "route53:ListResourceRecordSets"
                ],
                "Resource": [
                  "*"
                ]
              }
            ]
          }
          EOF
          
          # Create the policy
          aws iam create-policy \
            --policy-name ExternalDNSPolicy \
            --policy-document file://external-dns-policy.json || echo "Policy may already exist - continuing"
          
          # Create Service Account
          eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --namespace=kube-system \
            --name=external-dns \
            --attach-policy-arn=arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/ExternalDNSPolicy \
            --override-existing-serviceaccounts \
            --approve
            
          # Install External DNS
          helm repo add bitnami https://charts.bitnami.com/bitnami
          helm repo update
          
          helm install external-dns bitnami/external-dns \
            --set provider=aws \
            --set aws.region=$AWS_REGION \
            --set serviceAccount.create=false \
            --set serviceAccount.name=external-dns \
            --namespace kube-system
            
      - name: Create Kubernetes Namespace for Talk2Me
        run: |
          # Update kubectl config 
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
          
          # Create namespace
          kubectl apply -f k8s/namespace.yaml
          
      - name: Create Secret for DeepSeek API Key
        run: |
          kubectl create secret generic talk2me-secrets \
            --namespace talk2me \
            --from-literal=deepseek-api-key=${{ secrets.DEEPSEEK_API_KEY }}
            
      - name: Verify AWS Load Balancer Controller status
        run: |
          echo "Checking AWS Load Balancer Controller status..."
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          
          # Check controller logs
          echo "AWS Load Balancer Controller logs:"
          kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=50
          
      - name: Verify all add-ons status
        run: |
          echo "Verifying status of all EKS add-ons..."
          
          # Check status of all managed add-ons
          aws eks list-addons --cluster-name $CLUSTER_NAME --region $AWS_REGION
          
          # Verify all add-ons are in ACTIVE state
          ADDONS=$(aws eks list-addons --cluster-name $CLUSTER_NAME --region $AWS_REGION --query "addons" --output text)
          for ADDON in $ADDONS; do
            STATUS=$(aws eks describe-addon --cluster-name $CLUSTER_NAME --addon-name $ADDON --region $AWS_REGION --query "addon.status" --output text)
            echo "$ADDON status: $STATUS"
            if [ "$STATUS" != "ACTIVE" ]; then
              echo "Warning: $ADDON is not in ACTIVE state. Current state: $STATUS"
            fi
          done
          
          # Verify CoreDNS
          echo "Verifying CoreDNS pods..."
          kubectl get pods -n kube-system -l k8s-app=kube-dns
          
          # Verify kube-proxy
          echo "Verifying kube-proxy pods..."
          kubectl get pods -n kube-system -l k8s-app=kube-proxy
          
          # Verify vpc-cni
          echo "Verifying aws-node (vpc-cni) pods..."
          kubectl get pods -n kube-system -l k8s-app=aws-node
          
          # Verify EBS CSI Driver if installed
          if kubectl get pods -n kube-system -l app=ebs-csi-controller 2>/dev/null | grep -q ebs-csi-controller; then
            echo "Verifying EBS CSI Driver pods..."
            kubectl get pods -n kube-system -l app=ebs-csi-controller
          fi
          
      - name: Configure Cluster Autoscaler (optional)
        if: ${{ github.event.inputs.enable_autoscaler == 'true' }}
        run: |
          echo "Setting up Cluster Autoscaler..."
          
          # Create IAM policy for Cluster Autoscaler
          cat > cluster-autoscaler-policy.json << EOF
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": [
                  "autoscaling:DescribeAutoScalingGroups",
                  "autoscaling:DescribeAutoScalingInstances",
                  "autoscaling:DescribeLaunchConfigurations",
                  "autoscaling:DescribeTags",
                  "autoscaling:SetDesiredCapacity",
                  "autoscaling:TerminateInstanceInAutoScalingGroup",
                  "ec2:DescribeLaunchTemplateVersions"
                ],
                "Resource": ["*"]
              }
            ]
          }
          EOF
          
          # Create the policy
          aws iam create-policy \
            --policy-name ClusterAutoscalerPolicy \
            --policy-document file://cluster-autoscaler-policy.json || echo "Policy may already exist - continuing"
          
          # Create Service Account
          eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --namespace=kube-system \
            --name=cluster-autoscaler \
            --attach-policy-arn=arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/ClusterAutoscalerPolicy \
            --override-existing-serviceaccounts \
            --approve
            
          # Install Cluster Autoscaler
          helm repo add autoscaler https://kubernetes.github.io/autoscaler
          helm repo update
          
          helm install cluster-autoscaler autoscaler/cluster-autoscaler \
            --set autoDiscovery.clusterName=$CLUSTER_NAME \
            --set awsRegion=$AWS_REGION \
            --set rbac.serviceAccount.create=false \
            --set rbac.serviceAccount.name=cluster-autoscaler \
            --namespace kube-system
            
          # Wait for Cluster Autoscaler to be ready
          echo "Waiting for Cluster Autoscaler to be ready..."
          kubectl rollout status deployment cluster-autoscaler -n kube-system --timeout=180s
          
      - name: Export kubeconfig for future workflows
        run: |
          # Create kubeconfig that can be used in subsequent workflows
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
          
          # Save the kubeconfig to GitHub Outputs for reference in other workflow jobs
          echo "kubeconfig_path=$HOME/.kube/config" >> $GITHUB_OUTPUT
          
          echo "EKS Cluster setup complete. The cluster is now ready for application deployments."
          echo "Cluster name: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo "All add-ons and controllers have been configured and validated."