name: 3. Deploy Application to EKS

on:
  workflow_dispatch:
    inputs:
      region:
        description: 'AWS Region'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name'
        required: true
        default: 'talk2me-cluster'
      image_tag:
        description: 'Docker Image Tag'
        required: true
        default: 'latest'
      clean_up_resources:
        description: 'Clean up existing resources first'
        required: false
        default: 'false'
        type: boolean

env:
  AWS_REGION: ${{ github.event.inputs.region }}
  CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
  IMAGE_TAG: ${{ github.event.inputs.image_tag }}
  DOMAIN_NAME: 'talk2me-gen-z.com'
  API_DOMAIN_NAME: 'api.talk2me-gen-z.com'
  UNIQUE_SUFFIX: ${{ github.run_id }}

permissions:
  id-token: write
  contents: read

jobs:
  deploy-application:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION
      
      - name: Get AWS Account ID
        id: get-aws-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> $GITHUB_ENV
      
      - name: Clean up existing resources
        if: ${{ github.event.inputs.clean_up_resources == 'true' }}
        run: |
          echo "Cleaning up existing resources..."
          
          # First, find the problematic load balancer
          echo "Looking for existing load balancer 'k8s-talk2me-talk2mei-6ef1613cb5'..."
          LB_ARN=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, 'k8s-talk2me-talk2mei')].LoadBalancerArn" --output text || echo "")
          
          if [ -n "$LB_ARN" ]; then
            echo "Found load balancer: $LB_ARN"
            
            # Get target groups for this LB
            TG_ARNS=$(aws elbv2 describe-target-groups --load-balancer-arn $LB_ARN --query 'TargetGroups[].TargetGroupArn' --output text || echo "")
            
            # Delete the load balancer
            echo "Deleting load balancer..."
            aws elbv2 delete-load-balancer --load-balancer-arn $LB_ARN
            echo "Waiting for load balancer to be deleted..."
            sleep 30
            
            # Delete target groups
            if [ -n "$TG_ARNS" ]; then
              echo "Deleting associated target groups..."
              for TG_ARN in $TG_ARNS; do
                echo "Deleting target group: $TG_ARN"
                aws elbv2 delete-target-group --target-group-arn $TG_ARN || echo "Failed to delete target group"
              done
            fi
          else
            echo "No existing load balancer found with name 'k8s-talk2me-talk2mei-6ef1613cb5'"
          fi
          
          # Now clean up the Kubernetes resources
          echo "Deleting existing ingress resource..."
          kubectl delete ingress talk2me-ingress -n talk2me --ignore-not-found=true
          
          echo "Clean up complete"
      
      - name: Deploy Backend
        run: |
          echo "Deploying backend service..."
          
          # Process template with environment variables
          if [ -f "k8s/backend-deployment.yaml" ]; then
            envsubst < k8s/backend-deployment.yaml > backend-deployment.yaml
            kubectl apply -f backend-deployment.yaml -n talk2me
          else
            echo "Warning: k8s/backend-deployment.yaml not found"
          fi
          
          if [ -f "k8s/backend-service.yaml" ]; then
            kubectl apply -f k8s/backend-service.yaml -n talk2me
          else
            # Create a default service if file doesn't exist
            cat << EOF | kubectl apply -f -
            apiVersion: v1
            kind: Service
            metadata:
              name: talk2me-backend
              namespace: talk2me
            spec:
              selector:
                app: talk2me-backend
              ports:
              - port: 80
                targetPort: 8000
              type: ClusterIP
            EOF
          fi
      
      - name: Deploy Frontend
        run: |
          echo "Deploying frontend service..."
          
          # Process template with environment variables
          if [ -f "k8s/frontend-deployment.yaml" ]; then
            envsubst < k8s/frontend-deployment.yaml > frontend-deployment.yaml
            kubectl apply -f frontend-deployment.yaml -n talk2me
          else
            echo "Warning: k8s/frontend-deployment.yaml not found"
          fi
          
          if [ -f "k8s/frontend-service.yaml" ]; then
            kubectl apply -f k8s/frontend-service.yaml -n talk2me
          else
            # Create a default service if file doesn't exist
            cat << EOF | kubectl apply -f -
            apiVersion: v1
            kind: Service
            metadata:
              name: talk2me-frontend
              namespace: talk2me
            spec:
              selector:
                app: talk2me-frontend
              ports:
              - port: 80
                targetPort: 80
              type: ClusterIP
            EOF
          fi
      
      - name: Get ACM certificate ARN
        id: get-certificate
        run: |
          echo "Finding ACM certificate for $DOMAIN_NAME..."
          
          # Try to find certificate for domain in ACM
          CERT_ARN=$(aws acm list-certificates --query "CertificateSummaryList[?contains(DomainName, '*.$DOMAIN_NAME') || DomainName=='$DOMAIN_NAME'].CertificateArn" --output text | head -1)
          
          if [ -n "$CERT_ARN" ]; then
            echo "Found certificate: $CERT_ARN"
            echo "CERTIFICATE_ARN=$CERT_ARN" >> $GITHUB_ENV
            echo "certificate_found=true" >> $GITHUB_OUTPUT
          else
            echo "No certificate found for $DOMAIN_NAME"
            echo "certificate_found=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Create unique ingress with new load balancer name
        run: |
          echo "Creating unique ingress with specific load balancer name..."
          
          # Use a unique load balancer name based on run ID
          LB_NAME="talk2me-lb-${{ env.UNIQUE_SUFFIX }}"
          echo "Using load balancer name: $LB_NAME"
          
          # Create a new ingress file
          cat << EOF > ingress.yaml
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: talk2me-ingress
            namespace: talk2me
            annotations:
              kubernetes.io/ingress.class: "alb"
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
              alb.ingress.kubernetes.io/load-balancer-name: "$LB_NAME"
              alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
              alb.ingress.kubernetes.io/ssl-redirect: '443'
              alb.ingress.kubernetes.io/ssl-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
              external-dns.alpha.kubernetes.io/hostname: "$DOMAIN_NAME,$API_DOMAIN_NAME"
              alb.ingress.kubernetes.io/tags: Environment=prod,Team=talk2me
          EOF
          
          # Add certificate ARN if found
          if [ "${{ steps.get-certificate.outputs.certificate_found }}" == "true" ]; then
            echo "    alb.ingress.kubernetes.io/certificate-arn: \"$CERTIFICATE_ARN\"" >> ingress.yaml
          fi
          
          # Add the rules
          cat << EOF >> ingress.yaml
          spec:
            rules:
            - host: $DOMAIN_NAME
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: talk2me-frontend
                      port:
                        number: 80
            - host: $API_DOMAIN_NAME
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: talk2me-backend
                      port:
                        number: 80
          EOF
          
          # Display the final ingress file
          echo "Final ingress configuration:"
          cat ingress.yaml
          
          # Apply the ingress
          kubectl apply -f ingress.yaml
      
      - name: Wait for deployments to be ready
        run: |
          echo "Waiting for backend deployment to be ready..."
          kubectl rollout status deployment/talk2me-backend -n talk2me --timeout=300s
          
          echo "Waiting for frontend deployment to be ready..."
          kubectl rollout status deployment/talk2me-frontend -n talk2me --timeout=300s
      
      - name: Wait for ingress and load balancer
        run: |
          echo "Waiting for ingress to get an address (this may take a few minutes)..."
          
          # Wait for the ingress to get an address
          ATTEMPTS=0
          MAX_ATTEMPTS=45  # ~15 minutes
          SLEEP_SECONDS=20
          
          while [ $ATTEMPTS -lt $MAX_ATTEMPTS ]; do
            LB_ADDRESS=$(kubectl get ingress talk2me-ingress -n talk2me -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
            
            if [ -n "$LB_ADDRESS" ]; then
              echo "Ingress load balancer is available at: $LB_ADDRESS"
              echo "LB_ADDRESS=$LB_ADDRESS" >> $GITHUB_ENV
              break
            fi
            
            echo "Waiting for load balancer address... Attempt $(($ATTEMPTS+1))/$MAX_ATTEMPTS"
            
            # Every few attempts, check AWS resources and ingress status
            if [ $(($ATTEMPTS % 5)) -eq 0 ]; then
              echo "-----------------------------------"
              echo "Checking ingress status:"
              kubectl describe ingress talk2me-ingress -n talk2me
              
              echo "-----------------------------------"
              echo "Checking AWS Load Balancer Controller logs:"
              kubectl logs -n kube-system deployment/aws-load-balancer-controller --tail=20 || echo "Failed to get logs"
              
              echo "-----------------------------------"
              echo "Checking for load balancer in AWS:"
              LB_NAME="talk2me-lb-${{ env.UNIQUE_SUFFIX }}"
              aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '$LB_NAME')].{Name:LoadBalancerName,DNSName:DNSName,State:State.Code}" --output table || echo "No matching load balancer found yet"
            fi
            
            ATTEMPTS=$((ATTEMPTS+1))
            sleep $SLEEP_SECONDS
          done
          
          if [ -z "$LB_ADDRESS" ]; then
            echo "Warning: Load balancer address not available after several attempts"
            kubectl get ingress talk2me-ingress -n talk2me -o yaml
            
            # Check AWS for the load balancer directly
            LB_NAME="talk2me-lb-${{ env.UNIQUE_SUFFIX }}"
            LB_INFO=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '$LB_NAME')]" --output json)
            
            if [ "$(echo "$LB_INFO" | jq 'length')" -gt 0 ]; then
              AWS_LB_DNS=$(echo "$LB_INFO" | jq -r '.[0].DNSName')
              echo "Found load balancer in AWS: $AWS_LB_DNS"
              echo "LB_ADDRESS=$AWS_LB_DNS" >> $GITHUB_ENV
            else
              echo "Could not find load balancer in AWS. Check AWS Load Balancer Controller logs."
              echo "AWS Load Balancer Controller logs:"
              kubectl logs -n kube-system deployment/aws-load-balancer-controller --tail=50 || echo "Failed to get logs"
            fi
          fi
      
      - name: Update Route53 DNS if needed
        if: env.LB_ADDRESS != ''
        run: |
          echo "Looking for Route53 hosted zone for $DOMAIN_NAME..."
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name $DOMAIN_NAME --max-items 1 --query 'HostedZones[0].Id' --output text | cut -d'/' -f 3)
          
          if [ -z "$HOSTED_ZONE_ID" ] || [ "$HOSTED_ZONE_ID" == "null" ]; then
            echo "No hosted zone found for $DOMAIN_NAME"
            echo "Creating new hosted zone..."
            
            ZONE_RESULT=$(aws route53 create-hosted-zone \
              --name $DOMAIN_NAME \
              --caller-reference "talk2me-$(date +%s)" \
              --hosted-zone-config Comment="Hosted zone for Talk2Me application")
            
            # Extract zone ID from response
            HOSTED_ZONE_ID=$(echo $ZONE_RESULT | jq -r '.HostedZone.Id' | cut -d/ -f3)
            echo "Created new hosted zone with ID: $HOSTED_ZONE_ID"
            
            # Output nameservers for domain configuration
            echo "Please configure your domain registrar with the following nameservers:"
            echo $ZONE_RESULT | jq -r '.DelegationSet.NameServers[]' | sed 's/^/  - /'
          else
            echo "Found hosted zone with ID: $HOSTED_ZONE_ID"
          fi
          
          if [ -n "$HOSTED_ZONE_ID" ]; then
            echo "Updating Route53 DNS records to point to: $LB_ADDRESS"
            
            # Get the ALB hosted zone ID
            LB_NAME="talk2me-lb-${{ env.UNIQUE_SUFFIX }}"
            ALB_HOSTED_ZONE_ID=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '$LB_NAME')].CanonicalHostedZoneId" --output text || echo "Z35SXDOTRQ7X7K")
            
            # If we couldn't get the hosted zone ID, use the default for us-east-1
            if [ -z "$ALB_HOSTED_ZONE_ID" ] || [ "$ALB_HOSTED_ZONE_ID" == "None" ]; then
              ALB_HOSTED_ZONE_ID="Z35SXDOTRQ7X7K"  # Default for us-east-1
            fi
            
            # Create JSON for DNS change batch
            cat > dns-changes.json << EOF
            {
              "Changes": [
                {
                  "Action": "UPSERT",
                  "ResourceRecordSet": {
                    "Name": "$DOMAIN_NAME",
                    "Type": "A",
                    "AliasTarget": {
                      "HostedZoneId": "$ALB_HOSTED_ZONE_ID",
                      "DNSName": "$LB_ADDRESS",
                      "EvaluateTargetHealth": true
                    }
                  }
                },
                {
                  "Action": "UPSERT",
                  "ResourceRecordSet": {
                    "Name": "$API_DOMAIN_NAME",
                    "Type": "A",
                    "AliasTarget": {
                      "HostedZoneId": "$ALB_HOSTED_ZONE_ID",
                      "DNSName": "$LB_ADDRESS",
                      "EvaluateTargetHealth": true
                    }
                  }
                }
              ]
            }
            EOF
            
            # Apply the DNS changes
            CHANGE_ID=$(aws route53 change-resource-record-sets \
              --hosted-zone-id $HOSTED_ZONE_ID \
              --change-batch file://dns-changes.json \
              --query 'ChangeInfo.Id' --output text)
            
            echo "DNS records have been updated for $DOMAIN_NAME and $API_DOMAIN_NAME"
            echo "DNS change ID: $CHANGE_ID"
            echo "DNS changes take time to propagate. Please allow 5-10 minutes."
          else
            echo "Failed to get or create hosted zone. DNS records not updated."
          fi
      
      - name: Summary
        run: |
          echo "=========== Deployment Summary ==========="
          echo "Application deployed to EKS cluster: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo "Image tag: $IMAGE_TAG"
          
          # Check all resources
          echo ""
          echo "Kubernetes resources:"
          echo "- Deployments:"
          kubectl get deployments -n talk2me
          
          echo "- Services:"
          kubectl get services -n talk2me
          
          echo "- Ingress:"
          kubectl get ingress -n talk2me
          
          echo "- Pods:"
          kubectl get pods -n talk2me
          
          # Show access information
          echo ""
          echo "Access Information:"
          if [ -n "$LB_ADDRESS" ]; then
            echo "Load Balancer Address: $LB_ADDRESS"
            echo "Frontend URL: https://$DOMAIN_NAME"
            echo "Backend API URL: https://$API_DOMAIN_NAME"
            echo "Direct ALB access: http://$LB_ADDRESS"
            echo ""
            echo "DNS records have been updated, but changes may take time to propagate."
            echo "If the application is not accessible immediately, please wait 5-10 minutes."
          else
            echo "Load Balancer Address: Not available"
            echo "Could not determine the load balancer address. Check the AWS Load Balancer Controller logs."
          fi
          
          echo "=========================================="