# Deploy to EKS with Complete SSL, DNS, and IAM Configuration
- name: Deploy to EKS with SSL, DNS, and IAM setup
  if: steps.cluster-check.outputs.exists == 'true'
  run: |
    echo "Deploying Kubernetes manifests with SSL, DNS, and IAM configuration..."
    # Set variables for templating
    export AWS_ACCOUNT_ID="${{ steps.aws-account.outputs.account_id }}"
    export AWS_REGION="${{ steps.vars.outputs.aws_region }}"
    export IMAGE_TAG="${{ steps.vars.outputs.image_tag }}"
    export NAMESPACE="${{ steps.vars.outputs.namespace }}"
    export REGISTRY="${{ steps.vars.outputs.registry || format('{0}.dkr.ecr.{1}.amazonaws.com', steps.aws-account.outputs.account_id, steps.vars.outputs.aws_region) }}"
    export DOMAIN="talk2me-gen-z.com"
    export API_DOMAIN="api.talk2me-gen-z.com"
    
    # Create deployments directory if it doesn't exist
    mkdir -p deployments

    # Step 1: Verify/Create SSL Certificate
    echo "Step 1: Verifying SSL Certificate..."
    CERT_ARN=""
    
    if [ -n "${{ secrets.CERTIFICATE_ARN }}" ]; then
      # Use provided certificate ARN if available
      CERT_ARN="${{ secrets.CERTIFICATE_ARN }}"
      echo "Using certificate ARN from secrets: ${CERT_ARN}"
    else
      # Check if certificate exists for domain
      echo "Checking if certificate exists for ${DOMAIN}..."
      WILDCARD_CERT=$(aws acm list-certificates --region ${AWS_REGION} --query "CertificateSummaryList[?contains(DomainName, '*.${DOMAIN}')].CertificateArn" --output text)
      EXACT_CERT=$(aws acm list-certificates --region ${AWS_REGION} --query "CertificateSummaryList[?DomainName=='${DOMAIN}'].CertificateArn" --output text)
      
      if [ -n "$WILDCARD_CERT" ]; then
        CERT_ARN="$WILDCARD_CERT"
        echo "Found wildcard certificate: ${CERT_ARN}"
      elif [ -n "$EXACT_CERT" ]; then
        CERT_ARN="$EXACT_CERT"
        echo "Found exact domain certificate: ${CERT_ARN}"
      else
        echo "No existing certificate found. Requesting new certificate..."
        # Request a new certificate
        CERT_ARN=$(aws acm request-certificate \
          --domain-name "*.${DOMAIN}" \
          --validation-method DNS \
          --subject-alternative-names "${DOMAIN}" "${API_DOMAIN}" \
          --region ${AWS_REGION} \
          --query CertificateArn --output text)
        
        echo "New certificate requested: ${CERT_ARN}"
        echo "For the certificate to be valid, please create DNS validation records."
        
        # Get validation records
        sleep 5 # Wait for certificate to be created
        VALIDATION_OPTIONS=$(aws acm describe-certificate \
          --certificate-arn ${CERT_ARN} \
          --region ${AWS_REGION} \
          --query "Certificate.DomainValidationOptions" \
          --output json)
        
        echo "DNS validation records required:"
        echo "${VALIDATION_OPTIONS}" | jq -r '.[] | "Domain: \(.DomainName)\nRecord Name: \(.ResourceRecord.Name)\nRecord Type: \(.ResourceRecord.Type)\nRecord Value: \(.ResourceRecord.Value)\n"'
        
        echo "WARNING: Certificate is not yet validated. Deployment will proceed with HTTP only until DNS validation is complete."
      fi
    fi
    
    # Step 2: Create OIDC provider for ALB controller if needed
    echo "Step 2: Setting up IAM for ALB controller..."
    # Check if ALB controller exists
    if ! kubectl get deployment -n kube-system aws-load-balancer-controller 2>/dev/null; then
      echo "ALB controller not found. Setting up prerequisites..."

      # Check if OIDC provider exists for the cluster
      CLUSTER_OIDC_PROVIDER=$(aws eks describe-cluster \
        --name ${{ steps.vars.outputs.eks_cluster_name }} \
        --region ${AWS_REGION} \
        --query "cluster.identity.oidc.issuer" \
        --output text | sed -e "s/^https:\/\///")

      # Check if the OIDC provider already exists in IAM
      OIDC_PROVIDER_EXISTS=$(aws iam list-open-id-connect-providers | grep $CLUSTER_OIDC_PROVIDER || echo "")
      
      if [ -z "$OIDC_PROVIDER_EXISTS" ]; then
        echo "Creating OIDC provider for EKS cluster..."
        aws eks associate-identity-provider-config \
          --cluster-name ${{ steps.vars.outputs.eks_cluster_name }} \
          --region ${AWS_REGION} \
          --oidc identityProviderConfig={"type"="oidc","identityProviderConfigName"="alb-controller"}
      else
        echo "OIDC provider already exists for cluster."
      fi
      
      # Create IAM policy for ALB Controller
      ALB_POLICY_ARN=""
      ALB_POLICY_EXISTS=$(aws iam list-policies --query "Policies[?PolicyName=='AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)
      
      if [ -z "$ALB_POLICY_EXISTS" ]; then
        echo "Creating IAM policy for ALB controller..."
        # Download policy document
        curl -o alb-controller-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
        
        # Create policy
        ALB_POLICY_ARN=$(aws iam create-policy \
          --policy-name AWSLoadBalancerControllerIAMPolicy \
          --policy-document file://alb-controller-policy.json \
          --query 'Policy.Arn' --output text)
          
        echo "Created ALB controller policy: ${ALB_POLICY_ARN}"
      else
        ALB_POLICY_ARN=$ALB_POLICY_EXISTS
        echo "Using existing ALB controller policy: ${ALB_POLICY_ARN}"
      fi
      
      # Create ServiceAccount for ALB controller
      echo "Creating ServiceAccount for ALB controller..."
      cat << EOF > deployments/alb-controller-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKSLoadBalancerControllerRole
EOF
      
      kubectl apply -f deployments/alb-controller-sa.yaml
      
      # Install ALB controller using Helm
      echo "Installing ALB controller using Helm..."
      # Add Helm repo
      helm repo add eks https://aws.github.io/eks-charts
      helm repo update
      
      # Install ALB controller
      helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
        -n kube-system \
        --set clusterName=${{ steps.vars.outputs.eks_cluster_name }} \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --set region=${AWS_REGION} \
        --set vpcId=$(aws eks describe-cluster \
          --name ${{ steps.vars.outputs.eks_cluster_name }} \
          --region ${AWS_REGION} \
          --query "cluster.resourcesVpcConfig.vpcId" \
          --output text)
          
      echo "Waiting for ALB controller to be ready..."
      kubectl rollout status deployment aws-load-balancer-controller -n kube-system --timeout=180s
    else
      echo "ALB controller already installed."
    fi
    
    # Step 3: Install ExternalDNS if needed
    echo "Step 3: Setting up ExternalDNS..."
    if ! kubectl get deployment -n kube-system external-dns 2>/dev/null; then
      echo "ExternalDNS not found. Setting up..."
      
      # Create IAM policy for ExternalDNS
      EXTERNAL_DNS_POLICY_ARN=""
      EXTERNAL_DNS_POLICY_EXISTS=$(aws iam list-policies --query "Policies[?PolicyName=='ExternalDNSPolicy'].Arn" --output text)
      
      if [ -z "$EXTERNAL_DNS_POLICY_EXISTS" ]; then
        echo "Creating IAM policy for ExternalDNS..."
        cat << EOF > external-dns-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "route53:ChangeResourceRecordSets"
      ],
      "Resource": [
        "arn:aws:route53:::hostedzone/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "route53:ListHostedZones",
        "route53:ListResourceRecordSets"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}
EOF
        
        # Create policy
        EXTERNAL_DNS_POLICY_ARN=$(aws iam create-policy \
          --policy-name ExternalDNSPolicy \
          --policy-document file://external-dns-policy.json \
          --query 'Policy.Arn' --output text)
          
        echo "Created ExternalDNS policy: ${EXTERNAL_DNS_POLICY_ARN}"
      else
        EXTERNAL_DNS_POLICY_ARN=$EXTERNAL_DNS_POLICY_EXISTS
        echo "Using existing ExternalDNS policy: ${EXTERNAL_DNS_POLICY_ARN}"
      fi
      
      # Create ServiceAccount for ExternalDNS
      echo "Creating ServiceAccount for ExternalDNS..."
      cat << EOF > deployments/external-dns-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/ExternalDNSRole
EOF
      
      kubectl apply -f deployments/external-dns-sa.yaml
      
      # Deploy ExternalDNS
      echo "Deploying ExternalDNS..."
      cat << EOF > deployments/external-dns.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: external-dns
  namespace: kube-system
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: external-dns
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.k8s.io/external-dns/external-dns:v0.13.5
        args:
        - --source=service
        - --source=ingress
        - --domain-filter=${DOMAIN}
        - --provider=aws
        - --aws-zone-type=public
        - --registry=txt
        - --txt-owner-id=my-cluster-identifier
      securityContext:
        fsGroup: 65534
EOF
      
      kubectl apply -f deployments/external-dns.yaml
      
      echo "Waiting for ExternalDNS to be ready..."
      kubectl rollout status deployment external-dns -n kube-system --timeout=180s
    else
      echo "ExternalDNS already installed."
    fi
    
    # Step 4: Find hosted zone ID for the domain
    echo "Step 4: Finding Route53 hosted zone for ${DOMAIN}..."
    HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='${DOMAIN}.' || Name=='${DOMAIN}'].Id" --output text | sed -e 's/\/hostedzone\///')
    
    if [ -z "$HOSTED_ZONE_ID" ]; then
      echo "WARNING: No hosted zone found for ${DOMAIN}. Route53 DNS records will not be created automatically."
      echo "Please create a Route53 hosted zone for ${DOMAIN} and set up DNS delegation."
    else
      echo "Found hosted zone ID: ${HOSTED_ZONE_ID}"
    fi
    
    # Step 5: Generate and deploy Kubernetes manifests
    echo "Step 5: Generating Kubernetes manifests..."
    
    # Generate backend deployment with correct indentation
    cat << EOF > deployments/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: talk2me-backend
  namespace: ${NAMESPACE}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: talk2me-backend
  template:
    metadata:
      labels:
        app: talk2me-backend
    spec:
      containers:
      - name: backend
        image: ${REGISTRY}/talk2me-backend:${IMAGE_TAG}
        ports:
        - containerPort: 8000
        env:
        - name: DEEPSEEK_API_KEY
          valueFrom:
            secretKeyRef:
              name: talk2me-secrets
              key: deepseek-api-key
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "250m"
            memory: "256Mi"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 20
---
apiVersion: v1
kind: Service
metadata:
  name: talk2me-backend
  namespace: ${NAMESPACE}
  annotations:
    external-dns.alpha.kubernetes.io/hostname: "${API_DOMAIN}"
spec:
  selector:
    app: talk2me-backend
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
EOF
    
    # Generate frontend deployment with correct indentation
    cat << EOF > deployments/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: talk2me-frontend
  namespace: ${NAMESPACE}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: talk2me-frontend
  template:
    metadata:
      labels:
        app: talk2me-frontend
    spec:
      containers:
      - name: frontend
        image: ${REGISTRY}/talk2me-frontend:${IMAGE_TAG}
        ports:
        - containerPort: 80
        env:
        - name: API_URL
          value: "https://${API_DOMAIN}"
        resources:
          limits:
            cpu: "300m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 20
---
apiVersion: v1
kind: Service
metadata:
  name: talk2me-frontend
  namespace: ${NAMESPACE}
  annotations:
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN}"
spec:
  selector:
    app: talk2me-frontend
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF
    
    # Configure ingress based on certificate availability
    if [ -n "$CERT_ARN" ]; then
      echo "Configuring ingress with SSL certificate..."
      # Generate ingress with TLS and correct indentation
      cat << EOF > deployments/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: talk2me-ingress
  namespace: ${NAMESPACE}
  annotations:
    kubernetes.io/ingress.class: "alb"
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/certificate-arn: "${CERT_ARN}"
    alb.ingress.kubernetes.io/ssl-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN},${API_DOMAIN}"
    alb.ingress.kubernetes.io/healthcheck-path: "/"
    alb.ingress.kubernetes.io/success-codes: "200,302"
    alb.ingress.kubernetes.io/group.name: "talk2me"
spec:
  rules:
  - host: ${DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-frontend
            port:
              number: 80
  - host: ${API_DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-backend
            port:
              number: 80
EOF
    else
      echo "Configuring ingress without SSL (HTTP only)..."
      # Generate simplified ingress with correct indentation
      cat << EOF > deployments/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: talk2me-ingress
  namespace: ${NAMESPACE}
  annotations:
    kubernetes.io/ingress.class: "alb"
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN},${API_DOMAIN}"
    alb.ingress.kubernetes.io/healthcheck-path: "/"
    alb.ingress.kubernetes.io/success-codes: "200,302"
    alb.ingress.kubernetes.io/group.name: "talk2me"
spec:
  rules:
  - host: ${DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-frontend
            port:
              number: 80
  - host: ${API_DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-backend
            port:
              number: 80
EOF
    fi
    
    # Step 6: Apply the Kubernetes manifests
    echo "Step 6: Applying Kubernetes manifests..."
    kubectl apply -f deployments/backend-deployment.yaml
    kubectl apply -f deployments/frontend-deployment.yaml
    kubectl apply -f deployments/ingress.yaml
    
    # Step 7: Wait for resources to be available
    echo "Step 7: Waiting for deployments to be ready..."
    kubectl rollout status deployment/talk2me-backend -n ${NAMESPACE} --timeout=180s
    kubectl rollout status deployment/talk2me-frontend -n ${NAMESPACE} --timeout=180s
    
    # Step 8: Get the ALB DNS name
    echo "Step 8: Getting ALB information..."
    sleep 10 # Wait for ingress to be created
    ALB_DNS=$(kubectl get ingress talk2me-ingress -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
    
    if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "Not available yet" ]; then
      echo "ALB DNS: ${ALB_DNS}"
      
      # Create DNS records if hosted zone exists and ExternalDNS is not used
      if [ -n "$HOSTED_ZONE_ID" ] && ! kubectl get deployment -n kube-system external-dns &>/dev/null; then
        echo "Creating Route53 DNS records manually..."
        
        # Create A record alias for main domain
        cat << EOF > dns-change.json
{
  "Changes": [
    {
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "${DOMAIN}",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "Z2FDTNDATAQYW2",
          "DNSName": "${ALB_DNS}",
          "EvaluateTargetHealth": true
        }
      }
    },
    {
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "${API_DOMAIN}",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "Z2FDTNDATAQYW2",
          "DNSName": "${ALB_DNS}",
          "EvaluateTargetHealth": true
        }
      }
    }
  ]
}
EOF
        
        aws route53 change-resource-record-sets --hosted-zone-id ${HOSTED_ZONE_ID} --change-batch file://dns-change.json
        echo "DNS records created/updated for ${DOMAIN} and ${API_DOMAIN}"
      else
        echo "DNS will be managed by ExternalDNS or needs to be configured manually."
        echo "Point ${DOMAIN} and ${API_DOMAIN} to ${ALB_DNS}"
      fi
    else
      echo "ALB DNS not available yet. Check status with: kubectl get ingress talk2me-ingress -n ${NAMESPACE}"
    fi
    
    # List all applied resources
    echo "Deployed resources in namespace ${NAMESPACE}:"
    kubectl get all,ingress -n ${NAMESPACE}
    
    echo "Deployment completed successfully!"