name: 3. Deploy to K8s

on:
  workflow_run:
    workflows: ["2. Build and Push Images"]
    types:
      - completed
    branches: [main]
  # Allow manual deployments
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Image tag to deploy (defaults to latest build)'
        required: false
        default: ''
      namespace:
        description: 'Kubernetes namespace to deploy to'
        required: false
        default: 'talk2me'
      run_verification:
        description: 'Run verification workflow after deployment'
        required: false
        default: 'true'
        type: boolean
      aws_region:
        description: 'AWS Region (override)'
        required: false
        default: ''
      eks_cluster_name:
        description: 'EKS Cluster Name (override)'
        required: false
        default: ''

jobs:
  deploy:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    permissions:
      id-token: write  # Required for OIDC auth
      contents: read
    
    steps:
      # Initial Setup Steps
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download artifacts from workflow
        if: ${{ github.event_name == 'workflow_run' }}
        uses: dawidd6/action-download-artifact@v2
        with:
          workflow: build.yml
          name: deployment-info
          path: ./deployment-info
          workflow_conclusion: success
          
      # AWS Configuration
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ github.event.inputs.aws_region || secrets.AWS_REGION }}
          mask-aws-account-id: false # Show account ID for better debugging

      - name: Verify AWS authentication
        run: |
          echo "Verifying AWS authentication:"
          aws sts get-caller-identity
          echo "AWS credentials successfully configured"
          
          # Also check ECR access
          echo "Verifying ECR access:"
          aws ecr get-authorization-token --region ${{ github.event.inputs.aws_region || secrets.AWS_REGION }}
          echo "ECR access verified"
          
          # List available EKS clusters for debugging
          echo "Available EKS clusters in region ${{ github.event.inputs.aws_region || secrets.AWS_REGION }}:"
          aws eks list-clusters --region ${{ github.event.inputs.aws_region || secrets.AWS_REGION }} || echo "Failed to list clusters, check permissions"

      # Kubernetes Tools Installation
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
          
      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      # Environment Configuration
      - name: Get AWS account ID
        id: aws-account
        run: |
          echo "account_id=$(aws sts get-caller-identity --query Account --output text)" >> $GITHUB_OUTPUT

      - name: Set image tag
        id: vars
        run: |
          # Set namespace
          if [ -n "${{ github.event.inputs.namespace }}" ]; then
            NAMESPACE="${{ github.event.inputs.namespace }}"
          else
            NAMESPACE="talk2me"
          fi
          echo "namespace=${NAMESPACE}" >> $GITHUB_OUTPUT
          
          # Set EKS cluster name
          if [ -n "${{ github.event.inputs.eks_cluster_name }}" ]; then
            EKS_CLUSTER_NAME="${{ github.event.inputs.eks_cluster_name }}"
          elif [ -n "${{ secrets.EKS_CLUSTER_NAME }}" ]; then
            EKS_CLUSTER_NAME="${{ secrets.EKS_CLUSTER_NAME }}"
          else
            echo "::error::EKS cluster name is not set. Please set the EKS_CLUSTER_NAME secret or provide it as an input."
            exit 1
          fi
          echo "eks_cluster_name=${EKS_CLUSTER_NAME}" >> $GITHUB_OUTPUT
          
          # Set AWS region
          if [ -n "${{ github.event.inputs.aws_region }}" ]; then
            AWS_REGION="${{ github.event.inputs.aws_region }}"
          elif [ -n "${{ secrets.AWS_REGION }}" ]; then
            AWS_REGION="${{ secrets.AWS_REGION }}"
          else
            echo "::error::AWS region is not set. Please set the AWS_REGION secret or provide it as an input."
            exit 1
          fi
          echo "aws_region=${AWS_REGION}" >> $GITHUB_OUTPUT
          
          # Set image tag
          if [ -n "${{ github.event.inputs.image_tag }}" ]; then
            echo "image_tag=${{ github.event.inputs.image_tag }}" >> $GITHUB_OUTPUT
            echo "Using manually specified image tag: ${{ github.event.inputs.image_tag }}"
          elif [ -f "./deployment-info/image_tag.txt" ]; then
            IMAGE_TAG=$(cat ./deployment-info/image_tag.txt)
            echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT
            echo "Using image tag from build workflow: ${IMAGE_TAG}"
            if [ -f "./deployment-info/build_time.txt" ]; then
              BUILD_TIME=$(cat ./deployment-info/build_time.txt)
              echo "Image was built at: ${BUILD_TIME}"
            fi
            if [ -f "./deployment-info/registry.txt" ]; then
              REGISTRY=$(cat ./deployment-info/registry.txt)
              echo "registry=${REGISTRY}" >> $GITHUB_OUTPUT
            fi
          else
            # Fallback to 'latest' if no tag is available
            echo "image_tag=latest" >> $GITHUB_OUTPUT
            echo "Warning: No image tag found, using 'latest' instead"
          fi

      # Cluster Validation
      - name: Check if EKS cluster exists
        id: cluster-check
        run: |
          # Try to describe the cluster
          if aws eks describe-cluster --name ${{ steps.vars.outputs.eks_cluster_name }} --region ${{ steps.vars.outputs.aws_region }} --query "cluster.name" &>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "EKS cluster ${{ steps.vars.outputs.eks_cluster_name }} exists in region ${{ steps.vars.outputs.aws_region }}"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "::error::EKS cluster ${{ steps.vars.outputs.eks_cluster_name }} does not exist in region ${{ steps.vars.outputs.aws_region }}"
            
            # List available clusters in the region
            echo "Available EKS clusters in region ${{ steps.vars.outputs.aws_region }}:"
            aws eks list-clusters --region ${{ steps.vars.outputs.aws_region }} || echo "Failed to list clusters, check permissions"
            
            # List available regions for debugging
            echo "Available regions with EKS service:"
            aws ec2 describe-regions --query "Regions[].RegionName" --output text || echo "Failed to list regions, check permissions"
            
            exit 1
          fi

      # Kubernetes Configuration
      - name: Update kubeconfig for EKS cluster
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          # Get current role identity for debugging
          echo "Current AWS identity:"
          aws sts get-caller-identity
          
          # Update kubeconfig
          echo "Updating kubeconfig for EKS cluster ${{ steps.vars.outputs.eks_cluster_name }}..."
          aws eks update-kubeconfig --region ${{ steps.vars.outputs.aws_region }} \
            --name ${{ steps.vars.outputs.eks_cluster_name }}
          
          # Test access to the cluster
          echo "Testing access to Kubernetes cluster:"
          kubectl cluster-info
          kubectl get nodes || {
            echo "::error::Failed to access Kubernetes cluster. Check AWS role permissions or network access."
            exit 1
          }

      # Namespace Setup
      - name: Ensure namespace exists
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          echo "Creating namespace ${{ steps.vars.outputs.namespace }} if it doesn't exist..."
          kubectl create namespace ${{ steps.vars.outputs.namespace }} --dry-run=client -o yaml | kubectl apply -f -

      # Secret Management
      - name: Update Kubernetes secrets
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          # Check if DEEPSEEK_API_KEY is set
          if [ -z "${{ secrets.DEEPSEEK_API_KEY }}" ]; then
            echo "::warning::DEEPSEEK_API_KEY secret is not set"
            DEEPSEEK_API_KEY_BASE64=$(echo -n "dummy-key" | base64 -w 0)
          else
            echo "Creating Kubernetes secret with DEEPSEEK_API_KEY..."
            DEEPSEEK_API_KEY_BASE64=$(echo -n "${{ secrets.DEEPSEEK_API_KEY }}" | base64 -w 0)
          fi
          
          # Create secret configuration
          cat << EOF > ./secret.yaml
          apiVersion: v1
          kind: Secret
          metadata:
            name: talk2me-secrets
            namespace: ${{ steps.vars.outputs.namespace }}
          type: Opaque
          data:
            deepseek-api-key: ${DEEPSEEK_API_KEY_BASE64}
          EOF
          
          # Apply the secret
          kubectl apply -f ./secret.yaml
          rm ./secret.yaml

      # Main Deployment Step
      - name: Deploy to EKS with SSL, DNS, and IAM setup
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          echo "Deploying Kubernetes manifests with SSL, DNS, and IAM configuration..."
          # Set variables for templating
          export AWS_ACCOUNT_ID="${{ steps.aws-account.outputs.account_id }}"
          export AWS_REGION="${{ steps.vars.outputs.aws_region }}"
          export IMAGE_TAG="${{ steps.vars.outputs.image_tag }}"
          export NAMESPACE="${{ steps.vars.outputs.namespace }}"
          export REGISTRY="${{ steps.vars.outputs.registry || format('{0}.dkr.ecr.{1}.amazonaws.com', steps.aws-account.outputs.account_id, steps.vars.outputs.aws_region) }}"
          export DOMAIN="talk2me-gen-z.com"
          export API_DOMAIN="api.talk2me-gen-z.com"
          
          # Create deployments directory if it doesn't exist
          mkdir -p deployments

          # Step 1: Verify/Create SSL Certificate
          echo "Step 1: Verifying SSL Certificate..."
          CERT_ARN=""
          
          if [ -n "${{ secrets.CERTIFICATE_ARN }}" ]; then
            # Use provided certificate ARN if available
            CERT_ARN="${{ secrets.CERTIFICATE_ARN }}"
            echo "Using certificate ARN from secrets: ${CERT_ARN}"
          else
            # Check if certificate exists for domain
            echo "Checking if certificate exists for ${DOMAIN}..."
            WILDCARD_CERT=$(aws acm list-certificates --region ${AWS_REGION} --query "CertificateSummaryList[?contains(DomainName, '*.${DOMAIN}')].CertificateArn" --output text)
            EXACT_CERT=$(aws acm list-certificates --region ${AWS_REGION} --query "CertificateSummaryList[?DomainName=='${DOMAIN}'].CertificateArn" --output text)
            
            if [ -n "$WILDCARD_CERT" ]; then
              CERT_ARN="$WILDCARD_CERT"
              echo "Found wildcard certificate: ${CERT_ARN}"
            elif [ -n "$EXACT_CERT" ]; then
              CERT_ARN="$EXACT_CERT"
              echo "Found exact domain certificate: ${CERT_ARN}"
            else
              echo "No existing certificate found. Requesting new certificate..."
              # Request a new certificate
              CERT_ARN=$(aws acm request-certificate \
                --domain-name "*.${DOMAIN}" \
                --validation-method DNS \
                --subject-alternative-names "${DOMAIN}" "${API_DOMAIN}" \
                --region ${AWS_REGION} \
                --query CertificateArn --output text)
              
              echo "New certificate requested: ${CERT_ARN}"
              echo "For the certificate to be valid, please create DNS validation records."
              
              # Get validation records
              sleep 5 # Wait for certificate to be created
              VALIDATION_OPTIONS=$(aws acm describe-certificate \
                --certificate-arn ${CERT_ARN} \
                --region ${AWS_REGION} \
                --query "Certificate.DomainValidationOptions" \
                --output json)
              
              echo "DNS validation records required:"
              echo "${VALIDATION_OPTIONS}" | jq -r '.[] | "Domain: \(.DomainName)\nRecord Name: \(.ResourceRecord.Name)\nRecord Type: \(.ResourceRecord.Type)\nRecord Value: \(.ResourceRecord.Value)\n"'
              
              echo "WARNING: Certificate is not yet validated. Deployment will proceed with HTTP only until DNS validation is complete."
            fi
          fi
          
          # Step 2: Create OIDC provider for ALB controller if needed
          echo "Step 2: Setting up IAM for ALB controller..."
          # Check if ALB controller exists
          if ! kubectl get deployment -n kube-system aws-load-balancer-controller 2>/dev/null; then
            echo "ALB controller not found. Setting up prerequisites..."

            # Check if OIDC provider exists for the cluster
            CLUSTER_OIDC_PROVIDER=$(aws eks describe-cluster \
              --name ${{ steps.vars.outputs.eks_cluster_name }} \
              --region ${AWS_REGION} \
              --query "cluster.identity.oidc.issuer" \
              --output text | sed -e "s/^https:\/\///")

            # Check if the OIDC provider already exists in IAM
            OIDC_PROVIDER_EXISTS=$(aws iam list-open-id-connect-providers | grep $CLUSTER_OIDC_PROVIDER || echo "")
            
            if [ -z "$OIDC_PROVIDER_EXISTS" ]; then
              echo "Creating OIDC provider for EKS cluster..."
              aws eks associate-identity-provider-config \
                --cluster-name ${{ steps.vars.outputs.eks_cluster_name }} \
                --region ${AWS_REGION} \
                --oidc identityProviderConfig={"type"="oidc","identityProviderConfigName"="alb-controller"}
            else
              echo "OIDC provider already exists for cluster."
            fi
            
            # Create IAM policy for ALB Controller
            ALB_POLICY_ARN=""
            ALB_POLICY_EXISTS=$(aws iam list-policies --query "Policies[?PolicyName=='AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)
            
            if [ -z "$ALB_POLICY_EXISTS" ]; then
              echo "Creating IAM policy for ALB controller..."
              # Download policy document
              curl -o alb-controller-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
              
              # Create policy
              ALB_POLICY_ARN=$(aws iam create-policy \
                --policy-name AWSLoadBalancerControllerIAMPolicy \
                --policy-document file://alb-controller-policy.json \
                --query 'Policy.Arn' --output text)
                
              echo "Created ALB controller policy: ${ALB_POLICY_ARN}"
            else
              ALB_POLICY_ARN=$ALB_POLICY_EXISTS
              echo "Using existing ALB controller policy: ${ALB_POLICY_ARN}"
            fi
            
            # Create ServiceAccount for ALB controller
            echo "Creating ServiceAccount for ALB controller..."
            cat << EOF > deployments/alb-controller-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKSLoadBalancerControllerRole
EOF
            
            kubectl apply -f deployments/alb-controller-sa.yaml
            
            # Install ALB controller using Helm
            echo "Installing ALB controller using Helm..."
            # Add Helm repo
            helm repo add eks https://aws.github.io/eks-charts
            helm repo update
            
            # Install ALB controller
            helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
              -n kube-system \
              --set clusterName=${{ steps.vars.outputs.eks_cluster_name }} \
              --set serviceAccount.create=false \
              --set serviceAccount.name=aws-load-balancer-controller \
              --set region=${AWS_REGION} \
              --set vpcId=$(aws eks describe-cluster \
                --name ${{ steps.vars.outputs.eks_cluster_name }} \
                --region ${AWS_REGION} \
                --query "cluster.resourcesVpcConfig.vpcId" \
                --output text)
                
            echo "Waiting for ALB controller to be ready..."
            kubectl rollout status deployment aws-load-balancer-controller -n kube-system --timeout=180s
          else
            echo "ALB controller already installed."
          fi
          
          # Step 3: Install ExternalDNS if needed
          echo "Step 3: Setting up ExternalDNS..."
          if ! kubectl get deployment -n kube-system external-dns 2>/dev/null; then
            echo "ExternalDNS not found. Setting up..."
            
            # Create IAM policy for ExternalDNS
            EXTERNAL_DNS_POLICY_ARN=""
            EXTERNAL_DNS_POLICY_EXISTS=$(aws iam list-policies --query "Policies[?PolicyName=='ExternalDNSPolicy'].Arn" --output text)
            
            if [ -z "$EXTERNAL_DNS_POLICY_EXISTS" ]; then
              echo "Creating IAM policy for ExternalDNS..."
              cat << EOF > external-dns-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "route53:ChangeResourceRecordSets"
      ],
      "Resource": [
        "arn:aws:route53:::hostedzone/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "route53:ListHostedZones",
        "route53:ListResourceRecordSets"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}
EOF
              
              # Create policy
              EXTERNAL_DNS_POLICY_ARN=$(aws iam create-policy \
                --policy-name ExternalDNSPolicy \
                --policy-document file://external-dns-policy.json \
                --query 'Policy.Arn' --output text)
                
              echo "Created ExternalDNS policy: ${EXTERNAL_DNS_POLICY_ARN}"
            else
              EXTERNAL_DNS_POLICY_ARN=$EXTERNAL_DNS_POLICY_EXISTS
              echo "Using existing ExternalDNS policy: ${EXTERNAL_DNS_POLICY_ARN}"
            fi
            
            # Create ServiceAccount for ExternalDNS
            echo "Creating ServiceAccount for ExternalDNS..."
            cat << EOF > deployments/external-dns-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: external-dns
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/ExternalDNSRole
EOF
            
            kubectl apply -f deployments/external-dns-sa.yaml
            
            # Deploy ExternalDNS
            echo "Deploying ExternalDNS..."
            cat << EOF > deployments/external-dns.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: external-dns
  namespace: kube-system
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: external-dns
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.k8s.io/external-dns/external-dns:v0.13.5
        args:
        - --source=service
        - --source=ingress
        - --domain-filter=${DOMAIN}
        - --provider=aws
        - --aws-zone-type=public
        - --registry=txt
        - --txt-owner-id=my-cluster-identifier
      securityContext:
        fsGroup: 65534
EOF
            
            kubectl apply -f deployments/external-dns.yaml
            
            echo "Waiting for ExternalDNS to be ready..."
            kubectl rollout status deployment external-dns -n kube-system --timeout=180s
          else
            echo "ExternalDNS already installed."
          fi
          
          # Step 4: Find hosted zone ID for the domain
          echo "Step 4: Finding Route53 hosted zone for ${DOMAIN}..."
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='${DOMAIN}.' || Name=='${DOMAIN}'].Id" --output text | sed -e 's/\/hostedzone\///')
          
          if [ -z "$HOSTED_ZONE_ID" ]; then
            echo "WARNING: No hosted zone found for ${DOMAIN}. Route53 DNS records will not be created automatically."
            echo "Please create a Route53 hosted zone for ${DOMAIN} and set up DNS delegation."
          else
            echo "Found hosted zone ID: ${HOSTED_ZONE_ID}"
          fi
          
          # Step 5: Generate Kubernetes manifests
          echo "Step 5: Generating Kubernetes manifests..."
          
          # Generate backend deployment
          cat << EOF > deployments/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: talk2me-backend
  namespace: ${NAMESPACE}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: talk2me-backend
  template:
    metadata:
      labels:
        app: talk2me-backend
    spec:
      containers:
      - name: backend
        image: ${REGISTRY}/talk2me-backend:${IMAGE_TAG}
        ports:
        - containerPort: 8000
        env:
        - name: DEEPSEEK_API_KEY
          valueFrom:
            secretKeyRef:
              name: talk2me-secrets
              key: deepseek-api-key
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "250m"
            memory: "256Mi"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 20
---
apiVersion: v1
kind: Service
metadata:
  name: talk2me-backend
  namespace: ${NAMESPACE}
  annotations:
    external-dns.alpha.kubernetes.io/hostname: "${API_DOMAIN}"
spec:
  selector:
    app: talk2me-backend
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
EOF
          
          # Generate frontend deployment
          cat << EOF > deployments/frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: talk2me-frontend
  namespace: ${NAMESPACE}
spec:
  replicas: 2
  selector:
    matchLabels:
      app: talk2me-frontend
  template:
    metadata:
      labels:
        app: talk2me-frontend
    spec:
      containers:
      - name: frontend
        image: ${REGISTRY}/talk2me-frontend:${IMAGE_TAG}
        ports:
        - containerPort: 80
        env:
        - name: API_URL
          value: "https://${API_DOMAIN}"
        resources:
          limits:
            cpu: "300m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 20
---
apiVersion: v1
kind: Service
metadata:
  name: talk2me-frontend
  namespace: ${NAMESPACE}
  annotations:
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN}"
spec:
  selector:
    app: talk2me-frontend
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
EOF
          
          # Configure ingress based on certificate availability
          if [ -n "$CERT_ARN" ]; then
            echo "Configuring ingress with SSL certificate..."
            # Generate ingress with TLS
            cat << EOF > deployments/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: talk2me-ingress
  namespace: ${NAMESPACE}
  annotations:
    kubernetes.io/ingress.class: "alb"
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/certificate-arn: "${CERT_ARN}"
    alb.ingress.kubernetes.io/ssl-policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN},${API_DOMAIN}"
    alb.ingress.kubernetes.io/healthcheck-path: "/"
    alb.ingress.kubernetes.io/success-codes: "200,302"
    alb.ingress.kubernetes.io/group.name: "talk2me"
spec:
  rules:
  - host: ${DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-frontend
            port:
              number: 80
  - host: ${API_DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-backend
            port:
              number: 80
EOF
          else
            echo "Configuring ingress without SSL (HTTP only)..."
            # Generate simplified ingress
            cat << EOF > deployments/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: talk2me-ingress
  namespace: ${NAMESPACE}
  annotations:
    kubernetes.io/ingress.class: "alb"
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    external-dns.alpha.kubernetes.io/hostname: "${DOMAIN},${API_DOMAIN}"
    alb.ingress.kubernetes.io/healthcheck-path: "/"
    alb.ingress.kubernetes.io/success-codes: "200,302"
    alb.ingress.kubernetes.io/group.name: "talk2me"
spec:
  rules:
  - host: ${DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-frontend
            port:
              number: 80
  - host: ${API_DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: talk2me-backend
            port:
              number: 80
EOF
          fi
          
          # Step 6: Apply the Kubernetes manifests
          echo "Step 6: Applying Kubernetes manifests..."
          kubectl apply -f deployments/backend-deployment.yaml
          kubectl apply -f deployments/frontend-deployment.yaml
          kubectl apply -f deployments/ingress.yaml
          
          # Step 7: Wait for resources to be available
          echo "Step 7: Waiting for deployments to be ready..."
          kubectl rollout status deployment/talk2me-backend -n ${NAMESPACE} --timeout=180s
          kubectl rollout status deployment/talk2me-frontend -n ${NAMESPACE} --timeout=180s
          
          # Step 8: Get the ALB DNS name
          echo "Step 8: Getting ALB information..."
          sleep 10 # Wait for ingress to be created
          ALB_DNS=$(kubectl get ingress talk2me-ingress -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
          
          if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "Not available yet" ]; then
            echo "ALB DNS: ${ALB_DNS}"
            
            # Create DNS records if hosted zone exists and ExternalDNS is not used
            if [ -n "$HOSTED_ZONE_ID" ] && ! kubectl get deployment -n kube-system external-dns &>/dev/null; then
              echo "Creating Route53 DNS records manually..."
              
              # Create A record alias for main domain
              cat << EOF > dns-change.json
{
  "Changes": [
    {
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "${DOMAIN}",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "Z2FDTNDATAQYW2",
          "DNSName": "${ALB_DNS}",
          "EvaluateTargetHealth": true
        }
      }
    },
    {
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "${API_DOMAIN}",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "Z2FDTNDATAQYW2",
          "DNSName": "${ALB_DNS}",
          "EvaluateTargetHealth": true
        }
      }
    }
  ]
}
EOF
              
              aws route53 change-resource-record-sets --hosted-zone-id ${HOSTED_ZONE_ID} --change-batch file://dns-change.json
                echo "DNS records created/updated for ${DOMAIN} and ${API_DOMAIN}"
              else
                echo "DNS will be managed by ExternalDNS or needs to be configured manually."
                echo "Point ${DOMAIN} and ${API_DOMAIN} to ${ALB_DNS}"
              fi
            else
              echo "ALB DNS not available yet. Check status with: kubectl get ingress talk2me-ingress -n ${NAMESPACE}"
            fi
            
            # List all applied resources
            echo "Deployed resources in namespace ${NAMESPACE}:"
            kubectl get all,ingress -n ${NAMESPACE}
            
            echo "Deployment completed successfully!"
  
      # Verification Steps
      - name: Verify deployment
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          echo "Verifying backend deployment..."
          kubectl rollout status deployment/talk2me-backend -n ${{ steps.vars.outputs.namespace }} --timeout=180s || {
            echo "Warning: Backend deployment verification timed out"
            echo "Current pods status:"
            kubectl get pods -n ${{ steps.vars.outputs.namespace }} -l app=talk2me-backend -o wide
            echo "Recent pod events:"
            kubectl get events -n ${{ steps.vars.outputs.namespace }} --sort-by='.lastTimestamp' | grep backend || true
          }
            
          echo "Verifying frontend deployment..."
          kubectl rollout status deployment/talk2me-frontend -n ${{ steps.vars.outputs.namespace }} --timeout=180s || {
            echo "Warning: Frontend deployment verification timed out"
            echo "Current pods status:"
            kubectl get pods -n ${{ steps.vars.outputs.namespace }} -l app=talk2me-frontend -o wide
            echo "Recent pod events:"
            kubectl get events -n ${{ steps.vars.outputs.namespace }} --sort-by='.lastTimestamp' | grep frontend || true
          }
  
      # Service Information
      - name: Get service information
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          echo "Service information for namespace ${{ steps.vars.outputs.namespace }}:"
          kubectl get services -n ${{ steps.vars.outputs.namespace }}
            
          # Check if ingress is available
          if kubectl get ingress -n ${{ steps.vars.outputs.namespace }} &>/dev/null; then
            echo "Ingress information:"
            kubectl get ingress -n ${{ steps.vars.outputs.namespace }}
            echo "Application should be accessible via the above ingress address"
              
            # Get ALB address
            ALB_DNS=$(kubectl get ingress talk2me-ingress -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
            if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "Not available yet" ]; then
              echo "ALB DNS: ${ALB_DNS}"
              echo "You can access the application at:"
              echo "  Frontend: https://talk2me-gen-z.com"
              echo "  API: https://api.talk2me-gen-z.com"
              echo "Once DNS records have propagated"
            fi
          else
            echo "No ingress found. The application may be accessible via ClusterIP or LoadBalancer services."
          fi
            
          echo "Deployment completed at $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
            
      # Endpoint Checks
      - name: Check service endpoints
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          echo "Checking service endpoints in namespace ${{ steps.vars.outputs.namespace }}:"
          kubectl get services -n ${{ steps.vars.outputs.namespace }} -o wide
            
          echo "Service endpoints:"
          kubectl get endpoints -n ${{ steps.vars.outputs.namespace }}
            
          # Check if backend service has endpoints
          BACKEND_ENDPOINTS=$(kubectl get endpoints talk2me-backend -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.subsets[*].addresses}')
          if [ -n "$BACKEND_ENDPOINTS" ]; then
            echo "Service talk2me-backend has endpoints"
          else
            echo "::warning::Service talk2me-backend has no endpoints!"
          fi
            
          # Check if frontend service has endpoints
          FRONTEND_ENDPOINTS=$(kubectl get endpoints talk2me-frontend -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.subsets[*].addresses}')
          if [ -n "$FRONTEND_ENDPOINTS" ]; then
            echo "Service talk2me-frontend has endpoints"
          else
            echo "::warning::Service talk2me-frontend has no endpoints!"
          fi
            
      # Results and Artifacts
      - name: Save deployment results
        if: steps.cluster-check.outputs.exists == 'true'
        run: |
          # Get ALB address
          ALB_DNS=$(kubectl get ingress talk2me-ingress -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "Not available yet")
            
          # Create deployment summary
          cat << EOF > deployment-summary.md
          # Deployment Summary
            
          ## Details
          - Namespace: ${{ steps.vars.outputs.namespace }}
          - Image Tag: ${{ steps.vars.outputs.image_tag }}
          - Deployed At: $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          - Domain: talk2me-gen-z.com
          - API Domain: api.talk2me-gen-z.com
            
          ## Load Balancer
          - ALB DNS: ${ALB_DNS}
            
          ## Resources
          \`\`\`
          $(kubectl get pods,svc,ingress -n ${{ steps.vars.outputs.namespace }})
          \`\`\`
            
          ## Access Information
          - Frontend URL: https://talk2me-gen-z.com
          - API URL: https://api.talk2me-gen-z.com
          - Direct ALB URL: http://${ALB_DNS}
            
          ## Deployment Status
          - Backend: $(kubectl get deployment talk2me-backend -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
          - Frontend: $(kubectl get deployment talk2me-frontend -n ${{ steps.vars.outputs.namespace }} -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
            
          ## Service Endpoints
          \`\`\`
          $(kubectl get endpoints -n ${{ steps.vars.outputs.namespace }})
          \`\`\`
          EOF
  
      - name: Upload deployment results
        if: steps.cluster-check.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: deployment-results
          path: deployment-summary.md
          retention-days: 7
            
      # Failure Handling
      - name: Create failure summary
        if: steps.cluster-check.outputs.exists != 'true'
        run: |
          echo "## Deployment Failed" >> $GITHUB_STEP_SUMMARY
          echo "The EKS cluster **${{ steps.vars.outputs.eks_cluster_name }}** was not found in region **${{ steps.vars.outputs.aws_region }}**." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Troubleshooting Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. Verify the cluster name is correct" >> $GITHUB_STEP_SUMMARY
          echo "2. Check that the cluster exists in the specified AWS region" >> $GITHUB_STEP_SUMMARY
          echo "3. Ensure the GitHub Actions role has permission to access EKS" >> $GITHUB_STEP_SUMMARY
          echo "4. Try running the workflow again with the correct cluster name and region" >> $GITHUB_STEP_SUMMARY
            
          # Create a failure artifact as well
          cat << EOF > deployment-failure.md
          # Deployment Failed
            
          The EKS cluster **${{ steps.vars.outputs.eks_cluster_name }}** was not found in region **${{ steps.vars.outputs.aws_region }}**.
            
          ## AWS Identity Used
          \`\`\`
          $(aws sts get-caller-identity)
          \`\`\`
            
          ## Available EKS Clusters
          \`\`\`
          $(aws eks list-clusters --region ${{ steps.vars.outputs.aws_region }} 2>&1 || echo "Failed to list clusters")
          \`\`\`
            
          ## Troubleshooting Steps
          1. Verify the cluster name is correct
          2. Check that the cluster exists in the specified AWS region
          3. Ensure the GitHub Actions role has permission to access EKS
          4. Try running the workflow again with the correct cluster name and region
          EOF
            
      - name: Upload failure results
        if: steps.cluster-check.outputs.exists != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: deployment-failure
          path: deployment-failure.md
          retention-days: 7